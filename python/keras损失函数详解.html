<html><head><meta charset='utf-8'><meta name='viewport' content='width=device-width, initial-scale=1'>
<meta name='applicable-device' content='pc'><meta name='keywords' content='电脑,电脑讲解,电脑技术,编程,电脑故障维修keras损失函数详解' />
<script src='../../highlight/highlight.pack.js'></script>
<link rel='stylesheet' type='text/css' href='../../highlight/styles/monokai.css'/>

<link rel='stylesheet' href='../../fenxiang/dist/css/share.min.css'>
<script src='../../fenxiang/src/js/social-share.js'></script>
<script src='../../fenxiang/src/js/qrcode.js'></script>

</head><body><script>hljs.initHighlightingOnLoad();</script><script>
var system ={};  
var p = navigator.platform;       
system.win = p.indexOf('Win') == 0;  
system.mac = p.indexOf('Mac') == 0;  
system.x11 = (p == 'X11') || (p.indexOf('Linux') == 0);     
if(system.win||system.mac||system.xll){
document.write("<link href='../css/3.css' rel='stylesheet' type='text/css'>");}else{ document.write("<link href='../css/3wap.css' rel='stylesheet' type='text/css'>");}</script><script src='../../js/3.js'></script><div class='div2'><div class='heading_nav'><ul><div><li><a href='../../index.html'>首页</a></li>
</div><div onclick='hidden1()' >分享</div>
</ul></div></div>
<div id='heading_nav2'> 
<li class='row' >
<div class='social-share' data-mode='prepend'><a href='javascript:' class='social-share-icon icon-heart'></a></div></li></div><script charset='utf-8' src='../../3/js/hengfu.js'></script><script charset='utf-8' src='../../3/js/hengfu2.js'></script><hr><div class='div1'><div class='biaoti'><center>keras损失函数详解</center></div><div class='banquan'>原文出处:本文由博客园博主小小改变世界提供。<br/>
原文连接:https://www.cnblogs.com/gxxtsz/p/11253787.html</div><br>
    <h2>以下信息均来自官网</h2>
<p>------------------------------------------------------------------------------------------------------------</p>
<h2 id="_1">损失函数的使用</h2>
<p>损失函数（或称目标函数、优化评分函数）是编译模型时所需的两个参数之一：</p>
<div class="cnblogs_code">
<pre><code>model.compile(loss=<span style="color: #800000;">'</span><span style="color: #800000;">mean_squared_error</span><span style="color: #800000;">'</span>, optimizer=<span style="color: #800000;">'</span><span style="color: #800000;">sgd</span><span style="color: #800000;">'</span><span style="color: #000000;">)
</span><span style="color: #0000ff;">from</span> keras <span style="color: #0000ff;">import</span><span style="color: #000000;"> losses

model.compile(loss</span>=losses.mean_squared_error, optimizer=<span style="color: #800000;">'</span><span style="color: #800000;">sgd</span><span style="color: #800000;">'</span>)</pre>
</div>
<p>&nbsp;</p>
<p>你可以传递一个现有的损失函数名，或者一个 TensorFlow/Theano 符号函数。 该符号函数为每个数据点返回一个标量，有以下两个参数:</p>
<ul>
<li><strong>y_true</strong>: 真实标签。TensorFlow/Theano 张量。</li>
<li><strong>y_pred</strong>: 预测值。TensorFlow/Theano 张量，其 shape 与 y_true 相同。</li>
</ul>
<p>实际的优化目标是所有数据点的输出数组的平均值。</p>
<h2 id="_2">可用损失函数</h2>
<h3 id="mean_squared_error">mean_squared_error</h3>
<pre><code><code>mean_squared_error(y_true, y_pred)
</code></pre>
<hr />
<h3 id="mean_absolute_error">mean_absolute_error</h3>
<pre><code><code>mean_absolute_error(y_true, y_pred)
</code></pre>
<hr />
<h3 id="mean_absolute_percentage_error">mean_absolute_percentage_error</h3>
<pre><code><code>mean_absolute_percentage_error(y_true, y_pred)
</code></pre>
<hr />
<h3 id="mean_squared_logarithmic_error">mean_squared_logarithmic_error</h3>
<pre><code><code>mean_squared_logarithmic_error(y_true, y_pred)
</code></pre>
<hr />
<h3 id="squared_hinge">squared_hinge</h3>
<pre><code><code>squared_hinge(y_true, y_pred)
</code></pre>
<hr />
<h3 id="hinge">hinge</h3>
<pre><code><code>hinge(y_true, y_pred)
</code></pre>
<hr />
<h3 id="categorical_hinge">categorical_hinge</h3>
<pre><code><code>categorical_hinge(y_true, y_pred)
</code></pre>
<hr />
<h3 id="logcosh">logcosh</h3>
<pre><code><code>logcosh(y_true, y_pred)
</code></pre>
<p>预测误差的双曲余弦的对数。</p>
<p>对于小的&nbsp;<code>x</code>，<code>log(cosh(x))</code>&nbsp;近似等于&nbsp;<code>(x ** 2) / 2</code>。对于大的&nbsp;<code>x</code>，近似于&nbsp;<code>abs(x) - log(2)</code>。这表示 'logcosh' 与均方误差大致相同，但是不会受到偶尔疯狂的错误预测的强烈影响。</p>
<p><strong>参数</strong></p>
<ul>
<li><strong>y_true</strong>: 目标真实值的张量。</li>
<li><strong>y_pred</strong>: 目标预测值的张量。</li>
</ul>
<p><strong>返回</strong></p>
<p>每个样本都有一个标量损失的张量。</p>
<hr />
<h3 id="categorical_crossentropy">categorical_crossentropy</h3>
<pre><code><code>categorical_crossentropy(y_true, y_pred)
</code></pre>
<hr />
<h3 id="sparse_categorical_crossentropy">sparse_categorical_crossentropy</h3>
<pre><code><code>sparse_categorical_crossentropy(y_true, y_pred)
</code></pre>
<hr />
<h3 id="binary_crossentropy">binary_crossentropy</h3>
<pre><code><code>binary_crossentropy(y_true, y_pred)
</code></pre>
<hr />
<h3 id="kullback_leibler_divergence">kullback_leibler_divergence</h3>
<pre><code><code>kullback_leibler_divergence(y_true, y_pred)
</code></pre>
<hr />
<h3 id="poisson">poisson</h3>
<pre><code><code>poisson(y_true, y_pred)
</code></pre>
<hr />
<h3 id="cosine_proximity">cosine_proximity</h3>
<pre><code><code>cosine_proximity(y_true, y_pred)
</code></pre>
<hr />
<p><strong>注意</strong>: 当使用&nbsp;<code>categorical_crossentropy</code>&nbsp;损失时，你的目标值应该是分类格式 (即，如果你有 10 个类，每个样本的目标值应该是一个 10 维的向量，这个向量除了表示类别的那个索引为 1，其他均为 0)。 为了将&nbsp;<em>整数目标值</em>&nbsp;转换为&nbsp;<em>分类目标值</em>，你可以使用 Keras 实用函数&nbsp;<code>to_categorical</code>：</p>
<pre><code><code><span class="hljs-keyword">from keras.utils.np_utils <span class="hljs-keyword">import to_categorical

categorical_labels = to_categorical(int_labels, num_classes=<span class="hljs-keyword">None)<br /><br /><strong><span style="font-size: 18pt;">如果还不明白，请看下面的源码<br /></span></strong></span></span></span></code></pre>
<div class="cnblogs_code">
<pre><code><span style="color: #008080;">  1</span> <span style="color: #800000;">"""</span><span style="color: #800000;">Built-in loss functions.
</span><span style="color: #008080;">  2</span> <span style="color: #800000;">"""</span>
<span style="color: #008080;">  3</span> <span style="color: #0000ff;">from</span> <span style="color: #800080;">__future__</span> <span style="color: #0000ff;">import</span><span style="color: #000000;"> absolute_import
</span><span style="color: #008080;">  4</span> <span style="color: #0000ff;">from</span> <span style="color: #800080;">__future__</span> <span style="color: #0000ff;">import</span><span style="color: #000000;"> division
</span><span style="color: #008080;">  5</span> <span style="color: #0000ff;">from</span> <span style="color: #800080;">__future__</span> <span style="color: #0000ff;">import</span><span style="color: #000000;"> print_function
</span><span style="color: #008080;">  6</span> 
<span style="color: #008080;">  7</span> <span style="color: #0000ff;">import</span><span style="color: #000000;"> six
</span><span style="color: #008080;">  8</span> <span style="color: #0000ff;">from</span> . <span style="color: #0000ff;">import</span><span style="color: #000000;"> backend as K
</span><span style="color: #008080;">  9</span> <span style="color: #0000ff;">from</span> .utils.generic_utils <span style="color: #0000ff;">import</span><span style="color: #000000;"> deserialize_keras_object
</span><span style="color: #008080;"> 10</span> <span style="color: #0000ff;">from</span> .utils.generic_utils <span style="color: #0000ff;">import</span><span style="color: #000000;"> serialize_keras_object
</span><span style="color: #008080;"> 11</span> 
<span style="color: #008080;"> 12</span> 
<span style="color: #008080;"> 13</span> <span style="color: #0000ff;">def</span><span style="color: #000000;"> mean_squared_error(y_true, y_pred):
</span><span style="color: #008080;"> 14</span>     <span style="color: #0000ff;">return</span> K.mean(K.square(y_pred - y_true), axis=-1<span style="color: #000000;">)
</span><span style="color: #008080;"> 15</span> 
<span style="color: #008080;"> 16</span> 
<span style="color: #008080;"> 17</span> <span style="color: #0000ff;">def</span><span style="color: #000000;"> mean_absolute_error(y_true, y_pred):
</span><span style="color: #008080;"> 18</span>     <span style="color: #0000ff;">return</span> K.mean(K.abs(y_pred - y_true), axis=-1<span style="color: #000000;">)
</span><span style="color: #008080;"> 19</span> 
<span style="color: #008080;"> 20</span> 
<span style="color: #008080;"> 21</span> <span style="color: #0000ff;">def</span><span style="color: #000000;"> mean_absolute_percentage_error(y_true, y_pred):
</span><span style="color: #008080;"> 22</span>     diff = K.abs((y_true - y_pred) /<span style="color: #000000;"> K.clip(K.abs(y_true),
</span><span style="color: #008080;"> 23</span> <span style="color: #000000;">                                            K.epsilon(),
</span><span style="color: #008080;"> 24</span> <span style="color: #000000;">                                            None))
</span><span style="color: #008080;"> 25</span>     <span style="color: #0000ff;">return</span> 100. * K.mean(diff, axis=-1<span style="color: #000000;">)
</span><span style="color: #008080;"> 26</span> 
<span style="color: #008080;"> 27</span> 
<span style="color: #008080;"> 28</span> <span style="color: #0000ff;">def</span><span style="color: #000000;"> mean_squared_logarithmic_error(y_true, y_pred):
</span><span style="color: #008080;"> 29</span>     first_log = K.log(K.clip(y_pred, K.epsilon(), None) + 1<span style="color: #000000;">.)
</span><span style="color: #008080;"> 30</span>     second_log = K.log(K.clip(y_true, K.epsilon(), None) + 1<span style="color: #000000;">.)
</span><span style="color: #008080;"> 31</span>     <span style="color: #0000ff;">return</span> K.mean(K.square(first_log - second_log), axis=-1<span style="color: #000000;">)
</span><span style="color: #008080;"> 32</span> 
<span style="color: #008080;"> 33</span> 
<span style="color: #008080;"> 34</span> <span style="color: #0000ff;">def</span><span style="color: #000000;"> squared_hinge(y_true, y_pred):
</span><span style="color: #008080;"> 35</span>     <span style="color: #0000ff;">return</span> K.mean(K.square(K.maximum(1. - y_true * y_pred, 0.)), axis=-1<span style="color: #000000;">)
</span><span style="color: #008080;"> 36</span> 
<span style="color: #008080;"> 37</span> 
<span style="color: #008080;"> 38</span> <span style="color: #0000ff;">def</span><span style="color: #000000;"> hinge(y_true, y_pred):
</span><span style="color: #008080;"> 39</span>     <span style="color: #0000ff;">return</span> K.mean(K.maximum(1. - y_true * y_pred, 0.), axis=-1<span style="color: #000000;">)
</span><span style="color: #008080;"> 40</span> 
<span style="color: #008080;"> 41</span> 
<span style="color: #008080;"> 42</span> <span style="color: #0000ff;">def</span><span style="color: #000000;"> categorical_hinge(y_true, y_pred):
</span><span style="color: #008080;"> 43</span>     pos = K.sum(y_true * y_pred, axis=-1<span style="color: #000000;">)
</span><span style="color: #008080;"> 44</span>     neg = K.max((1. - y_true) * y_pred, axis=-1<span style="color: #000000;">)
</span><span style="color: #008080;"> 45</span>     <span style="color: #0000ff;">return</span> K.maximum(0., neg - pos + 1<span style="color: #000000;">.)
</span><span style="color: #008080;"> 46</span> 
<span style="color: #008080;"> 47</span> 
<span style="color: #008080;"> 48</span> <span style="color: #0000ff;">def</span><span style="color: #000000;"> logcosh(y_true, y_pred):
</span><span style="color: #008080;"> 49</span>     <span style="color: #800000;">"""</span><span style="color: #800000;">Logarithm of the hyperbolic cosine of the prediction error.
</span><span style="color: #008080;"> 50</span> <span style="color: #800000;">    `log(cosh(x))` is approximately equal to `(x ** 2) / 2` for small `x` and
</span><span style="color: #008080;"> 51</span> <span style="color: #800000;">    to `abs(x) - log(2)` for large `x`. This means that 'logcosh' works mostly
</span><span style="color: #008080;"> 52</span> <span style="color: #800000;">    like the mean squared error, but will not be so strongly affected by the
</span><span style="color: #008080;"> 53</span> <span style="color: #800000;">    occasional wildly incorrect prediction.
</span><span style="color: #008080;"> 54</span> <span style="color: #800000;">    # Arguments
</span><span style="color: #008080;"> 55</span> <span style="color: #800000;">        y_true: tensor of true targets.
</span><span style="color: #008080;"> 56</span> <span style="color: #800000;">        y_pred: tensor of predicted targets.
</span><span style="color: #008080;"> 57</span> <span style="color: #800000;">    # Returns
</span><span style="color: #008080;"> 58</span> <span style="color: #800000;">        Tensor with one scalar loss entry per sample.
</span><span style="color: #008080;"> 59</span>     <span style="color: #800000;">"""</span>
<span style="color: #008080;"> 60</span>     <span style="color: #0000ff;">def</span><span style="color: #000000;"> _logcosh(x):
</span><span style="color: #008080;"> 61</span>         <span style="color: #0000ff;">return</span> x + K.softplus(-2. * x) - K.log(2<span style="color: #000000;">.)
</span><span style="color: #008080;"> 62</span>     <span style="color: #0000ff;">return</span> K.mean(_logcosh(y_pred - y_true), axis=-1<span style="color: #000000;">)
</span><span style="color: #008080;"> 63</span> 
<span style="color: #008080;"> 64</span> 
<span style="color: #008080;"> 65</span> <span style="color: #0000ff;">def</span><span style="color: #000000;"> categorical_crossentropy(y_true, y_pred):
</span><span style="color: #008080;"> 66</span>     <span style="color: #0000ff;">return</span><span style="color: #000000;"> K.categorical_crossentropy(y_true, y_pred)
</span><span style="color: #008080;"> 67</span> 
<span style="color: #008080;"> 68</span> 
<span style="color: #008080;"> 69</span> <span style="color: #0000ff;">def</span><span style="color: #000000;"> sparse_categorical_crossentropy(y_true, y_pred):
</span><span style="color: #008080;"> 70</span>     <span style="color: #0000ff;">return</span><span style="color: #000000;"> K.sparse_categorical_crossentropy(y_true, y_pred)
</span><span style="color: #008080;"> 71</span> 
<span style="color: #008080;"> 72</span> 
<span style="color: #008080;"> 73</span> <span style="color: #0000ff;">def</span><span style="color: #000000;"> binary_crossentropy(y_true, y_pred):
</span><span style="color: #008080;"> 74</span>     <span style="color: #0000ff;">return</span> K.mean(K.binary_crossentropy(y_true, y_pred), axis=-1<span style="color: #000000;">)
</span><span style="color: #008080;"> 75</span> 
<span style="color: #008080;"> 76</span> 
<span style="color: #008080;"> 77</span> <span style="color: #0000ff;">def</span><span style="color: #000000;"> kullback_leibler_divergence(y_true, y_pred):
</span><span style="color: #008080;"> 78</span>     y_true = K.clip(y_true, K.epsilon(), 1<span style="color: #000000;">)
</span><span style="color: #008080;"> 79</span>     y_pred = K.clip(y_pred, K.epsilon(), 1<span style="color: #000000;">)
</span><span style="color: #008080;"> 80</span>     <span style="color: #0000ff;">return</span> K.sum(y_true * K.log(y_true / y_pred), axis=-1<span style="color: #000000;">)
</span><span style="color: #008080;"> 81</span> 
<span style="color: #008080;"> 82</span> 
<span style="color: #008080;"> 83</span> <span style="color: #0000ff;">def</span><span style="color: #000000;"> poisson(y_true, y_pred):
</span><span style="color: #008080;"> 84</span>     <span style="color: #0000ff;">return</span> K.mean(y_pred - y_true * K.log(y_pred + K.epsilon()), axis=-1<span style="color: #000000;">)
</span><span style="color: #008080;"> 85</span> 
<span style="color: #008080;"> 86</span> 
<span style="color: #008080;"> 87</span> <span style="color: #0000ff;">def</span><span style="color: #000000;"> cosine_proximity(y_true, y_pred):
</span><span style="color: #008080;"> 88</span>     y_true = K.l2_normalize(y_true, axis=-1<span style="color: #000000;">)
</span><span style="color: #008080;"> 89</span>     y_pred = K.l2_normalize(y_pred, axis=-1<span style="color: #000000;">)
</span><span style="color: #008080;"> 90</span>     <span style="color: #0000ff;">return</span> -K.sum(y_true * y_pred, axis=-1<span style="color: #000000;">)
</span><span style="color: #008080;"> 91</span> 
<span style="color: #008080;"> 92</span> 
<span style="color: #008080;"> 93</span> <span style="color: #008000;">#</span><span style="color: #008000;"> Aliases.</span>
<span style="color: #008080;"> 94</span> 
<span style="color: #008080;"> 95</span> mse = MSE =<span style="color: #000000;"> mean_squared_error
</span><span style="color: #008080;"> 96</span> mae = MAE =<span style="color: #000000;"> mean_absolute_error
</span><span style="color: #008080;"> 97</span> mape = MAPE =<span style="color: #000000;"> mean_absolute_percentage_error
</span><span style="color: #008080;"> 98</span> msle = MSLE =<span style="color: #000000;"> mean_squared_logarithmic_error
</span><span style="color: #008080;"> 99</span> kld = KLD =<span style="color: #000000;"> kullback_leibler_divergence
</span><span style="color: #008080;">100</span> cosine =<span style="color: #000000;"> cosine_proximity
</span><span style="color: #008080;">101</span> 
<span style="color: #008080;">102</span> 
<span style="color: #008080;">103</span> <span style="color: #0000ff;">def</span><span style="color: #000000;"> serialize(loss):
</span><span style="color: #008080;">104</span>     <span style="color: #0000ff;">return</span><span style="color: #000000;"> serialize_keras_object(loss)
</span><span style="color: #008080;">105</span> 
<span style="color: #008080;">106</span> 
<span style="color: #008080;">107</span> <span style="color: #0000ff;">def</span> deserialize(name, custom_objects=<span style="color: #000000;">None):
</span><span style="color: #008080;">108</span>     <span style="color: #0000ff;">return</span><span style="color: #000000;"> deserialize_keras_object(name,
</span><span style="color: #008080;">109</span>                                     module_objects=<span style="color: #000000;">globals(),
</span><span style="color: #008080;">110</span>                                     custom_objects=<span style="color: #000000;">custom_objects,
</span><span style="color: #008080;">111</span>                                     printable_module_name=<span style="color: #800000;">'</span><span style="color: #800000;">loss function</span><span style="color: #800000;">'</span><span style="color: #000000;">)
</span><span style="color: #008080;">112</span> 
<span style="color: #008080;">113</span> 
<span style="color: #008080;">114</span> <span style="color: #0000ff;">def</span><span style="color: #000000;"> get(identifier):
</span><span style="color: #008080;">115</span>     <span style="color: #800000;">"""</span><span style="color: #800000;">Get the `identifier` loss function.
</span><span style="color: #008080;">116</span> <span style="color: #800000;">    # Arguments
</span><span style="color: #008080;">117</span> <span style="color: #800000;">        identifier: None or str, name of the function.
</span><span style="color: #008080;">118</span> <span style="color: #800000;">    # Returns
</span><span style="color: #008080;">119</span> <span style="color: #800000;">        The loss function or None if `identifier` is None.
</span><span style="color: #008080;">120</span> <span style="color: #800000;">    # Raises
</span><span style="color: #008080;">121</span> <span style="color: #800000;">        ValueError if unknown identifier.
</span><span style="color: #008080;">122</span>     <span style="color: #800000;">"""</span>
<span style="color: #008080;">123</span>     <span style="color: #0000ff;">if</span> identifier <span style="color: #0000ff;">is</span><span style="color: #000000;"> None:
</span><span style="color: #008080;">124</span>         <span style="color: #0000ff;">return</span><span style="color: #000000;"> None
</span><span style="color: #008080;">125</span>     <span style="color: #0000ff;">if</span><span style="color: #000000;"> isinstance(identifier, six.string_types):
</span><span style="color: #008080;">126</span>         identifier =<span style="color: #000000;"> str(identifier)
</span><span style="color: #008080;">127</span>         <span style="color: #0000ff;">return</span><span style="color: #000000;"> deserialize(identifier)
</span><span style="color: #008080;">128</span>     <span style="color: #0000ff;">if</span><span style="color: #000000;"> isinstance(identifier, dict):
</span><span style="color: #008080;">129</span>         <span style="color: #0000ff;">return</span><span style="color: #000000;"> deserialize(identifier)
</span><span style="color: #008080;">130</span>     <span style="color: #0000ff;">elif</span><span style="color: #000000;"> callable(identifier):
</span><span style="color: #008080;">131</span>         <span style="color: #0000ff;">return</span><span style="color: #000000;"> identifier
</span><span style="color: #008080;">132</span>     <span style="color: #0000ff;">else</span><span style="color: #000000;">:
</span><span style="color: #008080;">133</span>         <span style="color: #0000ff;">raise</span> ValueError(<span style="color: #800000;">'</span><span style="color: #800000;">Could not interpret </span><span style="color: #800000;">'</span>
<span style="color: #008080;">134</span>                          <span style="color: #800000;">'</span><span style="color: #800000;">loss function identifier:</span><span style="color: #800000;">'</span>, identifier)</pre>
</div>
<pre><code><code></code></pre>
<p>&nbsp;</p>
<pre><code><code></code></pre>
<pre><code><code></code></pre>
<p>&nbsp;</p>
<pre><code><code><span class="hljs-keyword"><span class="hljs-keyword"><span class="hljs-keyword">&nbsp;</span></span></span></code></pre>
</div>
</div><hr><script charset='utf-8' src='../../js/sming.js'></script></body></html>